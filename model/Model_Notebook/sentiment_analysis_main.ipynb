{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Using IMDB Movie Data Set\n",
    "\n",
    "### Technologies Used:\n",
    "- Keras\n",
    "- LTSM RNN Model\n",
    "- Dataset: IMdB Movie Review Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset and importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "\n",
    "import numpy\n",
    "from numpy import array\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import load_model\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# fixing a random seed\n",
    "numpy.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading the IMdB dataset but limiting the vocabulary size to the first 5000 words\n",
    "top_words = 10000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews --> \n",
      "[1, 2, 365, 1234, 5, 1156, 354, 11, 14, 2, 2, 7, 1016, 2, 2, 356, 44, 4, 1349, 500, 746, 5, 200, 4, 4132, 11, 2, 2, 1117, 1831, 2, 5, 4831, 26, 6, 2, 4183, 17, 369, 37, 215, 1345, 143, 2, 5, 1838, 8, 1974, 15, 36, 119, 257, 85, 52, 486, 9, 6, 2, 2, 63, 271, 6, 196, 96, 949, 4121, 4, 2, 7, 4, 2212, 2436, 819, 63, 47, 77, 2, 180, 6, 227, 11, 94, 2494, 2, 13, 423, 4, 168, 7, 4, 22, 5, 89, 665, 71, 270, 56, 5, 13, 197, 12, 161, 2, 99, 76, 23, 2, 7, 419, 665, 40, 91, 85, 108, 7, 4, 2084, 5, 4773, 81, 55, 52, 1901]\n",
      "Labels --> \n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print('Reviews --> ') #Reviews are stored as a sequence of integers. Word IDs are preassigned to individual words\n",
    "print(X_train[6])\n",
    "print('Labels --> ') #Label is an integer 0 --> negative, 1 --> positive\n",
    "print(y_train[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review (Words) --> \n",
      "['the', 'and', 'full', 'involving', 'to', 'impressive', 'boring', 'this', 'as', 'and', 'and', 'br', 'villain', 'and', 'and', 'need', 'has', 'of', 'costumes', 'b', 'message', 'to', 'may', 'of', 'props', 'this', 'and', 'and', 'concept', 'issue', 'and', 'to', \"god's\", 'he', 'is', 'and', 'unfolds', 'movie', 'women', 'like', \"isn't\", 'surely', \"i'm\", 'and', 'to', 'toward', 'in', \"here's\", 'for', 'from', 'did', 'having', 'because', 'very', 'quality', 'it', 'is', 'and', 'and', 'really', 'book', 'is', 'both', 'too', 'worked', 'carl', 'of', 'and', 'br', 'of', 'reviewer', 'closer', 'figure', 'really', 'there', 'will', 'and', 'things', 'is', 'far', 'this', 'make', 'mistakes', 'and', 'was', \"couldn't\", 'of', 'few', 'br', 'of', 'you', 'to', \"don't\", 'female', 'than', 'place', 'she', 'to', 'was', 'between', 'that', 'nothing', 'and', 'movies', 'get', 'are', 'and', 'br', 'yes', 'female', 'just', 'its', 'because', 'many', 'br', 'of', 'overly', 'to', 'descent', 'people', 'time', 'very', 'bland']\n",
      "Label --> \n",
      "1\n"
     ]
    }
   ],
   "source": [
    "word2id = imdb.get_word_index() #Using the dictionary to map the WordIDs to original words\n",
    "id2word = {i: word for word, i in word2id.items()}\n",
    "print('Review (Words) --> ')\n",
    "print([id2word.get(i, ' ') for i in X_train[6]])\n",
    "print('Label --> ')\n",
    "print(y_train[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: fawn, ID: 34701\n",
      "Word: tsukino, ID: 52006\n",
      "Word: nunnery, ID: 52007\n",
      "Word: sonja, ID: 16816\n",
      "Word: vani, ID: 63951\n",
      "Word: woods, ID: 1408\n",
      "Word: spiders, ID: 16115\n",
      "Word: hanging, ID: 2345\n",
      "Word: woody, ID: 2289\n",
      "Word: trawling, ID: 52008\n",
      "Word: hold's, ID: 52009\n",
      "Word: comically, ID: 11307\n",
      "Word: localized, ID: 40830\n",
      "Word: disobeying, ID: 30568\n",
      "Word: 'royale, ID: 52010\n",
      "Word: harpo's, ID: 40831\n",
      "Word: canet, ID: 52011\n",
      "Word: aileen, ID: 19313\n",
      "Word: acurately, ID: 52012\n",
      "Word: diplomat's, ID: 52013\n"
     ]
    }
   ],
   "source": [
    "#Now we can view the word to ID correlation using word2id\n",
    "word2ID = list(word2id.items())[:20] #extracting the first 20 word pairs\n",
    "top_20_wordIDs = word2ID[:20] \n",
    "for word, id in top_20_wordIDs:\n",
    "    print(f\"Word: {word}, ID: {id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length: 2697\n",
      "Minimum length: 70\n"
     ]
    }
   ],
   "source": [
    "#getting the maximum and minimum length for the review\n",
    "print('Maximum length: {}'.format(\n",
    "len(max((X_train + X_test), key=len))))\n",
    "\n",
    "print('Minimum length: {}'.format(\n",
    "len(min((X_train + X_test), key=len))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limiting the length of maximum review to 500 for feeding into our RNN padding the shorter ones using pad_sequences()\n",
    "\n",
    "max_length = 600\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 600, 32)           320000    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 600, 32)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               53200     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 373301 (1.42 MB)\n",
      "Trainable params: 373301 (1.42 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "391/391 [==============================] - 317s 802ms/step - loss: 0.4784 - accuracy: 0.7659 - val_loss: 0.3822 - val_accuracy: 0.8472\n",
      "Epoch 2/50\n",
      "391/391 [==============================] - 308s 789ms/step - loss: 0.3151 - accuracy: 0.8735 - val_loss: 0.4269 - val_accuracy: 0.8375\n",
      "Epoch 3/50\n",
      "391/391 [==============================] - 310s 794ms/step - loss: 0.2605 - accuracy: 0.9002 - val_loss: 0.3200 - val_accuracy: 0.8655\n",
      "Epoch 4/50\n",
      "391/391 [==============================] - 310s 793ms/step - loss: 0.2318 - accuracy: 0.9099 - val_loss: 0.3053 - val_accuracy: 0.8718\n",
      "Epoch 5/50\n",
      "391/391 [==============================] - 310s 795ms/step - loss: 0.2193 - accuracy: 0.9165 - val_loss: 0.3321 - val_accuracy: 0.8643\n",
      "Epoch 6/50\n",
      "391/391 [==============================] - 312s 798ms/step - loss: 0.1995 - accuracy: 0.9232 - val_loss: 0.3241 - val_accuracy: 0.8693\n",
      "Epoch 7/50\n",
      "391/391 [==============================] - 310s 793ms/step - loss: 0.1699 - accuracy: 0.9375 - val_loss: 0.3291 - val_accuracy: 0.8720\n",
      "Epoch 8/50\n",
      "391/391 [==============================] - 310s 792ms/step - loss: 0.2067 - accuracy: 0.9191 - val_loss: 0.3512 - val_accuracy: 0.8733\n",
      "Epoch 9/50\n",
      "391/391 [==============================] - 309s 791ms/step - loss: 0.1803 - accuracy: 0.9297 - val_loss: 0.3737 - val_accuracy: 0.8435\n",
      "Epoch 10/50\n",
      "391/391 [==============================] - 310s 793ms/step - loss: 0.1311 - accuracy: 0.9532 - val_loss: 0.3967 - val_accuracy: 0.8684\n",
      "Epoch 11/50\n",
      "391/391 [==============================] - 311s 795ms/step - loss: 0.1390 - accuracy: 0.9499 - val_loss: 0.3995 - val_accuracy: 0.8624\n",
      "Epoch 12/50\n",
      "391/391 [==============================] - 308s 789ms/step - loss: 0.1196 - accuracy: 0.9578 - val_loss: 0.4250 - val_accuracy: 0.8667\n",
      "Epoch 13/50\n",
      "391/391 [==============================] - 311s 796ms/step - loss: 0.1000 - accuracy: 0.9650 - val_loss: 0.4800 - val_accuracy: 0.8658\n",
      "Epoch 14/50\n",
      "391/391 [==============================] - 309s 792ms/step - loss: 0.0916 - accuracy: 0.9684 - val_loss: 0.4980 - val_accuracy: 0.8659\n",
      "Epoch 15/50\n",
      "391/391 [==============================] - 311s 795ms/step - loss: 0.0865 - accuracy: 0.9693 - val_loss: 0.5227 - val_accuracy: 0.8637\n",
      "Epoch 16/50\n",
      "391/391 [==============================] - 311s 797ms/step - loss: 0.3022 - accuracy: 0.8898 - val_loss: 0.4131 - val_accuracy: 0.8598\n",
      "Epoch 17/50\n",
      "391/391 [==============================] - 310s 794ms/step - loss: 0.2223 - accuracy: 0.9153 - val_loss: 0.3896 - val_accuracy: 0.8627\n",
      "Epoch 18/50\n",
      "391/391 [==============================] - 313s 800ms/step - loss: 0.1167 - accuracy: 0.9588 - val_loss: 0.4489 - val_accuracy: 0.8672\n",
      "Epoch 19/50\n",
      "391/391 [==============================] - 312s 799ms/step - loss: 0.1109 - accuracy: 0.9609 - val_loss: 0.4580 - val_accuracy: 0.8610\n",
      "Epoch 20/50\n",
      "391/391 [==============================] - 312s 799ms/step - loss: 0.0807 - accuracy: 0.9721 - val_loss: 0.4892 - val_accuracy: 0.8616\n",
      "Epoch 21/50\n",
      "391/391 [==============================] - 313s 802ms/step - loss: 0.0705 - accuracy: 0.9764 - val_loss: 0.5275 - val_accuracy: 0.8575\n",
      "Epoch 22/50\n",
      "391/391 [==============================] - 311s 796ms/step - loss: 0.0697 - accuracy: 0.9772 - val_loss: 0.5673 - val_accuracy: 0.8634\n",
      "Epoch 23/50\n",
      "391/391 [==============================] - 314s 803ms/step - loss: 0.0816 - accuracy: 0.9727 - val_loss: 0.5341 - val_accuracy: 0.8635\n",
      "Epoch 24/50\n",
      "391/391 [==============================] - 313s 802ms/step - loss: 0.0591 - accuracy: 0.9799 - val_loss: 0.5582 - val_accuracy: 0.8576\n",
      "Epoch 25/50\n",
      "391/391 [==============================] - 314s 803ms/step - loss: 0.0529 - accuracy: 0.9830 - val_loss: 0.5746 - val_accuracy: 0.8552\n",
      "Epoch 26/50\n",
      "391/391 [==============================] - 313s 801ms/step - loss: 0.0479 - accuracy: 0.9848 - val_loss: 0.6591 - val_accuracy: 0.8624\n",
      "Epoch 27/50\n",
      "391/391 [==============================] - 311s 797ms/step - loss: 0.0659 - accuracy: 0.9785 - val_loss: 0.6086 - val_accuracy: 0.8596\n",
      "Epoch 28/50\n",
      "391/391 [==============================] - 314s 804ms/step - loss: 0.0471 - accuracy: 0.9849 - val_loss: 0.6949 - val_accuracy: 0.8570\n",
      "Epoch 29/50\n",
      "391/391 [==============================] - 313s 800ms/step - loss: 0.0540 - accuracy: 0.9829 - val_loss: 0.6716 - val_accuracy: 0.8631\n",
      "Epoch 30/50\n",
      "391/391 [==============================] - 313s 800ms/step - loss: 0.0631 - accuracy: 0.9793 - val_loss: 0.6277 - val_accuracy: 0.8644\n",
      "Epoch 31/50\n",
      "391/391 [==============================] - 314s 803ms/step - loss: 0.0490 - accuracy: 0.9848 - val_loss: 0.6224 - val_accuracy: 0.8609\n",
      "Epoch 32/50\n",
      "391/391 [==============================] - 312s 799ms/step - loss: 0.0451 - accuracy: 0.9868 - val_loss: 0.6234 - val_accuracy: 0.8597\n",
      "Epoch 33/50\n",
      "391/391 [==============================] - 314s 803ms/step - loss: 0.0382 - accuracy: 0.9884 - val_loss: 0.6523 - val_accuracy: 0.8595\n",
      "Epoch 34/50\n",
      "391/391 [==============================] - 312s 799ms/step - loss: 0.0284 - accuracy: 0.9917 - val_loss: 0.7054 - val_accuracy: 0.8594\n",
      "Epoch 35/50\n",
      "391/391 [==============================] - 312s 799ms/step - loss: 0.0402 - accuracy: 0.9866 - val_loss: 0.7176 - val_accuracy: 0.8608\n",
      "Epoch 36/50\n",
      "391/391 [==============================] - 313s 802ms/step - loss: 0.0408 - accuracy: 0.9870 - val_loss: 0.6656 - val_accuracy: 0.8596\n",
      "Epoch 37/50\n",
      "391/391 [==============================] - 312s 799ms/step - loss: 0.0376 - accuracy: 0.9882 - val_loss: 0.6686 - val_accuracy: 0.8578\n",
      "Epoch 38/50\n",
      "391/391 [==============================] - 314s 803ms/step - loss: 0.0366 - accuracy: 0.9888 - val_loss: 0.6929 - val_accuracy: 0.8540\n",
      "Epoch 39/50\n",
      "391/391 [==============================] - 314s 803ms/step - loss: 0.0330 - accuracy: 0.9904 - val_loss: 0.6549 - val_accuracy: 0.8603\n",
      "Epoch 40/50\n",
      "391/391 [==============================] - 313s 801ms/step - loss: 0.0304 - accuracy: 0.9910 - val_loss: 0.7655 - val_accuracy: 0.8654\n",
      "Epoch 41/50\n",
      "391/391 [==============================] - 315s 805ms/step - loss: 0.0326 - accuracy: 0.9898 - val_loss: 0.6947 - val_accuracy: 0.8598\n",
      "Epoch 42/50\n",
      "391/391 [==============================] - 312s 799ms/step - loss: 0.0249 - accuracy: 0.9924 - val_loss: 0.7542 - val_accuracy: 0.8626\n",
      "Epoch 43/50\n",
      "391/391 [==============================] - 315s 806ms/step - loss: 0.0343 - accuracy: 0.9892 - val_loss: 0.7404 - val_accuracy: 0.8615\n",
      "Epoch 44/50\n",
      "391/391 [==============================] - 314s 802ms/step - loss: 0.0212 - accuracy: 0.9936 - val_loss: 0.7308 - val_accuracy: 0.8587\n",
      "Epoch 45/50\n",
      "391/391 [==============================] - 312s 799ms/step - loss: 0.0245 - accuracy: 0.9926 - val_loss: 0.7230 - val_accuracy: 0.8650\n",
      "Epoch 46/50\n",
      "391/391 [==============================] - 314s 803ms/step - loss: 0.0648 - accuracy: 0.9796 - val_loss: 0.5642 - val_accuracy: 0.8575\n",
      "Epoch 47/50\n",
      "391/391 [==============================] - 313s 802ms/step - loss: 0.0401 - accuracy: 0.9876 - val_loss: 0.6699 - val_accuracy: 0.8582\n",
      "Epoch 48/50\n",
      "391/391 [==============================] - 314s 803ms/step - loss: 0.0301 - accuracy: 0.9906 - val_loss: 0.6655 - val_accuracy: 0.8590\n",
      "Epoch 49/50\n",
      "391/391 [==============================] - 314s 804ms/step - loss: 0.0274 - accuracy: 0.9922 - val_loss: 0.7301 - val_accuracy: 0.8661\n",
      "Epoch 50/50\n",
      "391/391 [==============================] - 313s 801ms/step - loss: 0.0165 - accuracy: 0.9957 - val_loss: 0.7736 - val_accuracy: 0.8622\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1eb5a956d50>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Model Creation\n",
    "\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_length))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  86.22400164604187\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: \", (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Sentiment Analysis\\.venv\\Lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model.save(\"sentiment_analysis.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"sentiment_analysis.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
